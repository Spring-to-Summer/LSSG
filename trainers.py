from logging.config import valid_ident

import torch

from transformers import Trainer, AutoConfig

from utils import print_rank_0, IGNORE_INDEX
from sentence_transformers import SentenceTransformer, util
from transformers import pipeline
from typing import List,Tuple
import os
from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification, pipeline
from deepspeed.runtime.zero.partitioned_param_coordinator import PartitionedParameterCoordinator
from deepspeed.runtime.zero.partition_parameters import ZeroParamStatus

def compute_lm_loglikeli(logits, labels):
    batch_size, seq_length, vocab_size = logits.shape

    shift_logits = logits[..., :-1, :].contiguous()
    shift_labels = labels[..., 1:].contiguous()

    # Flatten the tokens
    loss_fct = torch.nn.CrossEntropyLoss(reduction="none")
    shift_logits = shift_logits.view(-1, vocab_size)
    shift_labels = shift_labels.view(-1)

    # Enable model parallelism
    shift_labels = shift_labels.to(shift_logits.device)
    loss = loss_fct(shift_logits, shift_labels).reshape(
        batch_size, -1
    )  # [bs * seq_len]
    ignore_mask = labels != IGNORE_INDEX

    avg_loss = loss.sum(dim=-1) / ignore_mask.sum(dim=-1)

    return -avg_loss


class SFTWeightedWithKLTrainer(Trainer):

    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):
        if self.args.debug_mode:
            print_rank_0(f"check inputs :{inputs}")

        model_outputs = model(
            input_ids=inputs["input_ids"], attention_mask=inputs["attention_mask"]
        )

        with torch.no_grad():
            ref_model_outputs = model.ref_model(
                input_ids=inputs["input_ids"], attention_mask=inputs["attention_mask"]
            )
            ref_logprob = compute_lm_loglikeli(
                ref_model_outputs.logits, inputs["labels"]
            )  # [batch_size]

        if self.args.debug_mode:
            print_rank_0(f"check ref_model output: {ref_logprob}")

        logprob = compute_lm_loglikeli(model_outputs.logits, inputs["labels"])

        # for MC kl
        kl_divergence = logprob.exp() * (logprob - ref_logprob)

        loss = -logprob + self.args.lm_kl_coeff * kl_divergence

        total_loss = (loss * inputs["weights"]).mean()  # [batch_size]

        # æ–°å¢ Entropy æ­£åˆ™åŒ–é¡¹
        logits = model_outputs.logits  # [batch, seq_len, vocab_size]
        probs = torch.nn.functional.softmax(logits, dim=-1)
        entropy = - (probs * torch.log(probs + 1e-8)).sum(dim=-1)  # [batch, seq_len]
        avg_entropy = entropy.mean()

        # åŠ å…¥ entropy æ­£åˆ™é¡¹
        total_loss = total_loss - self.args.entropy_coeff * avg_entropy

        if self.args.debug_mode:
            print_rank_0(f"check loss : {loss}")
            print_rank_0(f"check total_loss : {total_loss}")
            print_rank_0(f"check kl divergence : {kl_divergence}")
            print_rank_0(f"check entropy : {avg_entropy}")

        return (total_loss, model_outputs) if return_outputs else total_loss

class DummySentenceEmbedder:
    def __init__(self, model_path):
        self.tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)
        self.model = AutoModel.from_pretrained(model_path, local_files_only=True)

    def encode(self, texts):
        inputs = self.tokenizer(texts, padding=True, truncation=True, return_tensors='pt')
        with torch.no_grad():
            outputs = self.model(**inputs)
            embeddings = outputs.last_hidden_state.mean(dim=1)
        return embeddings

class OfflineWeightedPolicyTrainer(Trainer):
    def __init__(self,sentiment_classifier, semantic_model, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.semantic_model=semantic_model
        self.sentiment_classifier=sentiment_classifier
        if isinstance(self.model, dict):
            self.model = self.model.get("model", self.model)

    def unwrap_and_sync_model(self, model):
        print(f"[Before unwrap] model = {type(model)}, has module = {hasattr(model, 'module')}")
        if hasattr(model, "module"):
            base_model = model.module
        else:
            base_model = model

        base_model.zero_pad_model_inputs = True

        print("â³ [DeepSpeed] Start manual parameter prefetch (recursive)")
        num_params = 0
        failed = 0

        try:
            for name, module in base_model.named_modules():  # recurse=True
                for param in module.parameters(recurse=False):
                    if hasattr(param, "ds_status"):
                        try:
                            # å¼ºåˆ¶è§¦å‘ param.data åŠ è½½
                            _ = param.data
                            num_params += 1
                            if hasattr(param, "param_coordinator") and param.param_coordinator is not None:
                                param.param_coordinator.fetch_sub_module(module, forward=True)
                        except Exception as e:
                            failed += 1
                            print(f"[Prefetch Failed] param in module '{name}' -> {e}")
        except Exception as e:
            print(f"[Fatal Error] Manual prefetch failed: {e}")

        print(f"âœ… [DeepSpeed] Manual parameter prefetch complete (recursive). Total: {num_params}, Failed: {failed}")
        return base_model

    def encode_texts(self, texts: List[str], fallback_shape: Tuple[int, int], device):
        try:
            cleaned = [t.strip() for t in texts if isinstance(t, str) and t.strip()]
            if not cleaned:
                return torch.zeros(fallback_shape, device=device)
            return self.semantic_model.encode(cleaned, convert_to_tensor=True, truncation=True, max_length=512).to(device)
        except Exception as e:
            print_rank_0(f"[Warning] Failed to encode texts: {e}")
            return torch.zeros(fallback_shape, device=device)

    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):
        # ğŸ’¥ ç¡®ä¿æ¨¡å‹ä¸æ˜¯ dict
        if isinstance(model, dict):
            print_rank_0("[Fix] 'model' is a dict, extracting actual model via model['model']")
            model = model.get("model", model)
        print_rank_0(f"[Debug] type(model) = {type(model)}")
        print_rank_0(f"[Debug] hasattr(model, 'config') = {hasattr(model, 'config')}")

        if self.args.debug_mode:
            print_rank_0(f"check inputs :{inputs}")
        model_outputs = model(
            input_ids=inputs["input_ids"], attention_mask=inputs["attention_mask"]
        )

        with torch.no_grad():
            ref_model_outputs = model.ref_model(
                input_ids=inputs["input_ids"], attention_mask=inputs["attention_mask"]
            )

            ref_logprob = compute_lm_loglikeli(
                ref_model_outputs.logits, inputs["labels"]
            ).detach()  # [batch_size]

        if self.args.debug_mode:
            print_rank_0(f"check ref_model output: {ref_logprob}")

        logprob = compute_lm_loglikeli(model_outputs.logits, inputs["labels"])
        kl_div = logprob - ref_logprob

        print("===============================================================================")
        print("input_ids.shape:", inputs["input_ids"].shape)
        print("attention_mask.shape:", inputs["attention_mask"].shape)

        print(f"[Before unwrap] model = {type(model)}, has module = {hasattr(model, 'module')}")

        # ä½¿ç”¨æ–°æ–¹æ³•ç»Ÿä¸€å¤„ç†
        model = self.unwrap_and_sync_model(model)
        max_input_len = model.config.max_position_embeddings - 64  # æ”¾åœ¨è§£åŒ…ä¹‹åå†è®¿é—®ï¼
        #  ç¡®ä¿ generate å‰æ‰€æœ‰å‚æ•°å·²æ‹‰å…¥å†…å­˜
        with torch.no_grad():
            _ = model(input_ids=inputs["input_ids"][:, -max_input_len:],
                      attention_mask=inputs["attention_mask"][:, -max_input_len:])
            torch.cuda.synchronize()

        was_training = model.training
        model.gradient_checkpointing_disable()
        model.config.use_cache = True
        # è¿™é‡Œç”Ÿæˆ generated_texts æ–‡æœ¬å¹¶ decode å‡ºæ¥
        with torch.no_grad():
            generated_ids = model.generate(
                input_ids=inputs["input_ids"][:, -max_input_len:],
                attention_mask = inputs["attention_mask"][:, -max_input_len:],
                # input_ids=inputs["input_ids"],
                # attention_mask=inputs["attention_mask"],
                max_new_tokens=64,
                do_sample=True,
                top_k=50,
                repetition_penalty=1.2,
                temperature=0.95
            )
        if was_training:
            model.train()
        model.gradient_checkpointing_enable()
        generated_texts = self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)
        print("generated_texts =", generated_texts)


        for i, text in enumerate(generated_texts):
            tokenized = self.semantic_model.tokenizer.encode(text, truncation=True)
            print(f"[{i}] len={len(tokenized)} text={repr(text[:100])}")

        importance_ratio = (logprob - ref_logprob).exp()
        importance_ratio_clipped = torch.clip(
            importance_ratio, 1 - self.args.clip_range, 1 + self.args.clip_range
        )

        advantages = inputs["rewards"] - self.args.lm_kl_coeff * kl_div
        ppo_loss = -torch.minimum(
            advantages * importance_ratio, advantages * importance_ratio_clipped
        )

        sample_size, sft_size = (1 - inputs["sft_mask"]).sum(), (
            inputs["sft_mask"]
        ).sum()
        sft_loss = (
            (-logprob * inputs["sft_mask"]).sum() / sft_size
            if sft_size > 0
            else sft_size
        )
        ppo_loss = (
            (ppo_loss * (1 - inputs["sft_mask"])).sum() / sample_size
            if sample_size > 0
            else sample_size
        )

        #  è‡ªå®šä¹‰æ–‡æœ¬ï¼šè¿˜éœ€è¦ inputs["ref_texts"] â†’ ç”¨ label decode å¾—åˆ°
        labels = inputs["labels"].clone()
        labels[labels == -100] = self.tokenizer.pad_token_id or 0
        reference_texts = self.tokenizer.batch_decode(labels, skip_special_tokens=True)

        # æ¸…æ´— valid_texts
        valid_texts = [t.strip() for t in generated_texts if isinstance(t, str) and t.strip()]
        dim = self.semantic_model.get_sentence_embedding_dimension()
        min_len = min(len(valid_texts), len(reference_texts))

        if min_len == 0:
            print_rank_0("[Warning] No valid texts for semantic comparison.")
            return torch.tensor(0.0, requires_grad=True, device=logprob.device)  # æˆ–ç›´æ¥ return sft_loss + ppo_loss

        # è®¡ç®—å‘é‡
        gen_emb = self.encode_texts(valid_texts[:min_len], (min_len, dim), logprob.device)
        ref_emb = self.encode_texts(reference_texts[:min_len], (min_len, dim), logprob.device)

        print(f"gen_emb.shape = {gen_emb.shape}")
        print(f"ref_emb.shape = {ref_emb.shape}")

        # è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦ï¼ˆé¼“åŠ±ä¸ ref ä¸å¤ªæ¥è¿‘ï¼Œä½†ä¸ input ä¿æŒè¯­ä¹‰ç›¸å…³æ€§ï¼‰
        cos_sim = util.pytorch_cos_sim(gen_emb, ref_emb).diagonal()  # è¶Šå¤§è¶Šç›¸ä¼¼
        semantic_diversity_reward = 1 - cos_sim  # è¶Šå¤§è¶Šä¸åŒ
        semantic_penalty = - self.args.semantic_coeff * semantic_diversity_reward.to(logprob.device)

        # æƒ…ç»ªé²æ£’æ€§ï¼šæƒ…ç»ªå¹³è¡¡æŸå¤±é¡¹
        sentiments = self.sentiment_classifier(generated_texts, truncation=True, max_length=512)

        # å¾—åˆ°æ¯å¥è¯çš„æ­£é¢æ¦‚ç‡ï¼Œä¾‹å¦‚ï¼š{'label': 'POSITIVE', 'score': 0.98}
        emotion_stability = torch.tensor(
            [s["score"] if s["label"] == "POSITIVE" else 1 - s["score"] for s in sentiments], device=logprob.device)
        # å‡è®¾ç›®æ ‡æƒ…ç»ªåä¸­æ€§ï¼Œæƒ©ç½šåç¦» 0.5 çš„ç¨‹åº¦ï¼š
        # emotion_penalty = (emotion_stability - 0.5).abs() # smoothç‰ˆæœ¬
        emotion_penalty = torch.square(emotion_stability - 0.5) # strong ç‰ˆæœ¬
        emotion_penalty = self.args.emotion_coeff * emotion_penalty.to(logprob.device)

        # åŠ æƒåŠ å…¥ PPO æŸå¤±ï¼Œå¯¹é½é•¿åº¦
        ppo_loss = ppo_loss.view(-1) if len(ppo_loss.shape) == 0 else ppo_loss
        semantic_penalty = semantic_penalty.view(-1) if len(semantic_penalty.shape) == 0 else semantic_penalty
        emotion_penalty = emotion_penalty.view(-1) if len(emotion_penalty.shape) == 0 else emotion_penalty
        min_len = min(ppo_loss.shape[0], semantic_penalty.shape[0], emotion_penalty.shape[0])

        ppo_loss = ppo_loss[:min_len]
        semantic_penalty = semantic_penalty[:min_len]
        emotion_penalty = emotion_penalty[:min_len]

        # total_loss æœ¬åº”æ˜¯ batch ä¸­æ¯ä¸ªæ ·æœ¬çš„ lossï¼Œsft_loss æ˜¯æ ‡é‡ï¼Œä¸éœ€è¦åˆ‡
        total_loss = self.args.lm_sft_coeff * sft_loss + ppo_loss + semantic_penalty + emotion_penalty

        # weighted_loss = (total_loss * inputs["weights"]).mean()  # [batch_size]
        weighted_loss = (total_loss * inputs["weights"][:min_len]).mean()

        if self.args.debug_mode:
            print_rank_0(f"check total_loss : {total_loss}")
            print_rank_0(f"check weighted loss : {weighted_loss}")
            print_rank_0(f"check kl divergence : {kl_div}")

        return (weighted_loss, model_outputs) if return_outputs else weighted_loss
